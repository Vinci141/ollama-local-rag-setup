# üß† Ollama Local RAG Setup
> Run a fully local Retrieval-Augmented Generation (RAG) system using Ollama ‚Äî index and query your own documents directly on your machine.

---

## üìò Overview
This project lets you build a **local knowledge assistant** powered by [Ollama](https://ollama.ai/).  
It reads and indexes your local files, creates embeddings, and enables **semantic search + Q&A** without sending data to the cloud.

‚úÖ **Everything runs locally** ‚Äî no external API keys or internet required.  
üß© Ideal for privacy-sensitive projects, research, and offline data exploration.

---

## ‚öôÔ∏è Features
- üîç **Automatic document indexing** with FAISS
- üß¨ **Embeddings** powered by `mxbai-embed-large` (Ollama)
- üí¨ **Natural language querying** over your local files
- üíæ **Offline support** ‚Äî runs entirely on your machine
- üß∞ **Modular design** ‚Äî easy to extend with other models or data sources

---

## üöÄ Getting Started

### 1Ô∏è‚É£ Prerequisites
- [Ollama](https://ollama.ai/download) installed and running locally  
  > Verify with:
  > ```bash
  > ollama serve
  > ```
- Python ‚â• 3.9
- `git`, `pip`, and a code editor (PyCharm, VS Code, etc.)

---

### 2Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/<your-username>/ollama-local-rag-setup.git
cd ollama-local-rag-setup
