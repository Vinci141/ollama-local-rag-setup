# üß† Ollama Local RAG Setup
> Run a fully local Retrieval-Augmented Generation (RAG) system using Ollama ‚Äî index and query your own documents directly on your machine.

---

## üìò Overview
This project lets you build a **local knowledge assistant** powered by [Ollama](https://ollama.ai/).  
It reads and indexes your local files, creates embeddings, and enables **semantic search + Q&A** without sending data to the cloud.

‚úÖ **Everything runs locally** ‚Äî no external API keys or internet required.  
üß© Ideal for privacy-sensitive projects, research, and offline data exploration.

---

## ‚öôÔ∏è Features
- üîç **Automatic document indexing** with FAISS
- üß¨ **Embeddings** powered by `mxbai-embed-large` (Ollama)
- üí¨ **Natural language querying** over your local files
- üíæ **Offline support** ‚Äî runs entirely on your machine
- üß∞ **Modular design** ‚Äî easy to extend with other models or data sources

---

## üöÄ Getting Started

### 1Ô∏è‚É£ Prerequisites
- [Ollama](https://ollama.ai/download) installed and running locally  
  > Verify with:
  > ```bash
  > ollama serve
  > ```
- Python ‚â• 3.9
- `git`, `pip`, and a code editor (PyCharm, VS Code, etc.)
  

---

### 2Ô∏è‚É£ Clone the Repository
    bash
    git clone https://github.com/<your-username>/ollama-local-rag-setup.git
    cd ollama-local-rag-setup

###3Ô∏è‚É£ Create a Virtual Environment

    python -m venv .venv
    .\.venv\Scripts\activate      # On Windows
    source .venv/bin/activate     # On macOS / Linux

###4Ô∏è‚É£ Install Dependencies

    pip install -r requirements.txt

###5Ô∏è‚É£ Pull Required Ollama Models

    ollama pull mxbai-embed-large
    ollama pull llama3

6Ô∏è‚É£ Index Your Local Files

python Ollama_setup.py --index --folder "C:\path\to\your\docs"
  This step:

    Reads files from the given folder
    Generates embeddings
    Builds a FAISS vector index for fast retrieval
7Ô∏è‚É£ Ask Questions About Your Files

python 02_Ollama_setup.py --query "Summarize project goals"

üìÇ Project Structure

    ollama-local-rag-setup/
    ‚îÇ
    ‚îú‚îÄ‚îÄ Ollama_setup.py      # Main script for indexing and querying
    ‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
    ‚îú‚îÄ‚îÄ data/                    # Folder for source documents [Optionally use this in arguement -->C:\path\to\your\docs]
    ‚îú‚îÄ‚îÄ index/                   # Stored FAISS vector database
    ‚îî‚îÄ‚îÄ README.md


    Content of requirement.txt

    # Core Ollama client
    ollama>=0.1.0
    
    # Vector database and numeric operations
    faiss-cpu>=1.7.4
    numpy>=1.24.0

    # PDF processing
    pypdf>=3.17.0

    # Note: These are already included in Python standard library
    # - os
    # - glob
    # - json
    # - typing
    # - argparse

üß© Architecture

    User Query
      ‚Üì
    Ollama LLM (e.g., Llama 3)
      ‚Üì
    Relevant Context Retrieved from FAISS
      ‚Üì
    Embedded via mxbai-embed-large
      ‚Üì
    Response Generated ‚Üí Displayed to User


üí° Example Output

    > python 02_Ollama_setup.py --query "What is this document about?"

Answer:
This document describes the internal architecture and deployment setup
for the RAG-based Ollama local environment...

üß∞ Tech Stack

| Component             | Description                          |
| --------------------- | ------------------------------------ |
| **Ollama**            | Local LLM runtime                    |
| **mxbai-embed-large** | Embedding model for document vectors |
| **FAISS**             | Vector search engine                 |
| **Python**            | Glue logic for indexing and querying |


üîí Privacy

All processing happens locally ‚Äî no data leaves your machine.
Perfect for confidential files, research notes, or enterprise data.
